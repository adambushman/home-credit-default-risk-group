---
title: "Modeling for Default"
subtitle: "Modeling default for loan applications with Support Vector Machines"
author: "Adam Bushman"
date: "2024-10-20"
format:
    html:
        embed-resources: true
execute:
    warning: false
---


# Approach

I'm focusing on modeling for `DEFAULT` using **Support Vector Machines**.

A suport vector machine could be a good approach due to the nature of the problem and data:

*   Many features
*   Capturing some non-linear relationships
*   Robustness to overfitting 
*   Binary classification


## Prep

Let's load a few libraries:

```{r}
library('tidyverse')
library('e1071')
library('caret')
library('pROC')
```

We'll load the cleaned, balanced, training data:

```{r}
path <- 'D:/All Repos/home-credit-default-risk-group/data/application_train_smote.csv'

data <- data.table::fread(path) |>
    as.data.frame() |> 
    mutate(DEFAULT = factor(DEFAULT, levels = c("N", "Y")))

glimpse(data)
```

All features in this file may conceivably be helpful in predicting `DEFAULT` (originally named `TARGET`), with exception of `SK_ID_CURR`. Additionally, we need to remove all categorical variables that have just 1 class (essentially, constants).

We do have a class imbalance to work out: 

We'll work with a sample of the data since we have over 300K records.

```{r}
set.seed(2015)

# Splits
partition_idx <- createDataPartition(data$DEFAULT, p = 0.8, list = FALSE)
full_train <- data[partition_idx,]
full_test <- data[-partition_idx,]

# 1% sample of the data
train_sampl <- sample_n(
    full_train, ceiling(nrow(full_train) * 0.01)
)
test_sampl <- sample_n(
    full_test, ceiling(nrow(full_test) * 0.05)
)
```

```{r}
unique <- sapply(train_sampl, function(x) length(unique(x)))
remove_cols <- names(unique[unique == 1])

train_sampl <- train_sampl |> select(-all_of(remove_cols))
test_sampl <- test_sampl |> select(-all_of(remove_cols))
```

All features in this file may conceivably be helpful in predicting `DEFAULT` (originally named `TARGET`) so we shouldn't need to subset any of them.

## Support Vector machine

We also want to setup our cross validation, which includes a hyperparameter tuning grid and a training control object:

```{r}
fitControl <- trainControl(
    method = "repeatedcv", 
    number = 5, 
    repeats = 2, 
    classProbs = TRUE, 
    summaryFunction = twoClassSummary, 
    savePredictions = TRUE
)

fitGrid <- expand.grid(
    sigma = c(0.01, 0.05), 
    C = c(0.01, 0.5, 1, 5, 10)
)
```


## Basic Modeling

We can now setup a cross-validated training of `DEFAULT` with the SMOTE data:

```{r}
mod03 <- train(
    DEFAULT ~ ., 
    data = train_sampl, 
    method = "svmRadial", 
    trControl = fitControl, 
    metric = "ROC",  # Use ROC for AUC
    tuneGrid = fitGrid
)

mod03
```

The AUC values are pretty poor. At around 0.56, that's not much better than a random guess or using the majority class. 

Let's now generate full metrics from the best model we've got here. Fisrt, we'll use the model details we found before:

```{r}
tuned_mod <- e1071::svm(DEFAULT ~ ., data = train_sampl, sigma = 0.01, C = 0.05)
```

```{r}
pred_probs <- predict(tuned_mod, newdata = test_sampl, type = 'response')
```

Now, we need to find the ideal threshold for classification of the probabilities:

```{r}
#| warning: false
roc_obj <- roc(ifelse(test_sampl$DEFAULT == "Y", 1, 0), ifelse(pred_probs == "Y", 1, 0))
youden_index <- roc_obj$sensitivities + roc_obj$specificities - 1

optimal_index <- which.max(youden_index)
optimal_threshold <- roc_obj$thresholds[optimal_index]
optimal_threshold
```

We can now use this optimal threshold for the confusion matrix:

```{r}
mat3 <- confusionMatrix(
    factor(ifelse(pred_probs == "Y", 1, 0), levels = c(1,0)), 
    factor(ifelse(test_sampl$DEFAULT == "Y", 1, 0), levels = c(1,0))
)

mat3
```

Let's also grab the AUC value:

```{r}
auc3 <- auc(roc_obj)
auc3
```

Let's compile all these metrics into a dataframe and save the results:

```{r}
performance <- c(
    c("model" = "Support Vector Machine"), 
    c("hyperparameters" = paste(
        paste("Sigma:", 0.1), 
        paste("C:", 10.0), 
        collapse = ", "
    )), 
    mat3$overall[c("Accuracy")], 
    mat3$byClass[c("Precision", "Recall")], 
    c("AUC" = auc3)
)

performance
```

```{r}
#| ignore: true
#| eval: false
data.frame(as.list(performance)) |>
    write.csv('models/support-vector-machine/model-results.csv', row.names = FALSE)
```

## Conclusion

Despite all of my research, I can't figure out why the cross validation AUC is so poor but the AUC with the testing data is so good. It has to be an issue with the cross validation using caret. 